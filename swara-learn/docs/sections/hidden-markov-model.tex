\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Hidden Markov Models}
\begin{definition}
A \textbf{hidden markov model} models a system in which every sequence of observations $O_1, \ldots, O_t$ are generated by a Markov process whose state $H_t$ is hidden from the observer. A hidden markov model learns three probability distributions: transition $T = P(H_{t+1} | H_{t})$, emission $E = P(O_{t} | H_{t})$, and initial $I = P(H_{0})$. Hidden markov models are especially useful for sequence prediction (e.g. speech-to-text) and generation (e.g. speech generation).
\end{definition}

\subsection{Prediction}
Given a sequence of observations $O_1, \ldots, O_n$ find the most probable sequence of hidden states $H_1, \ldots, H_n$ that generated them.

\begin{definition}
A \textbf{path} $P$ is a sequence of hidden states $P_1, \ldots, P_i$. For any path $P$, let $len(P) = i$ be the length of the path and $prob(P)$ be the probability of the path such that:
\begin{equation*}
  prob(P)=\begin{cases}
    I(P_1) * E(O_0, P_1), & \text{if $len(P) = 1$}.\\
    prob(P_1, \ldots, P_{i-1}) * T(P_i, P_{i-1}) * E(O_i, P_i), & \text{otherwise}.
  \end{cases}
\end{equation*}
\end{definition}

\subsubsection{Algorithm}
\begin{algorithm}[H]
\caption{Predict the most probable sequence of hidden states $H_1, \ldots, H_n$ that generated a given sequence of observed states $O_1, \ldots, O_n$}
\label{hmm-predict}
\begin{algorithmic}
\Function{Predict}{$O_1, \ldots, O_n$}
    \State $M = $ max-heap of paths keyed by probability
    \State $V = $ set of visited paths
    \ForAll{$H \in I$ s.t. $I(H) > 0$}
        \State $enqueue(H, M)$
    \EndFor
    \While{$R = dequeue(M)$ and $len(R) <= n$}
        \If{$R \not\in V$}
            \ForAll{$H \in T(H, R_{len(R)}) > 0$}
                \State $enqueue(R \cup H, M)$
                \State $V = V \cup R_{len(R)}$
            \EndFor
        \EndIf
    \EndWhile
    \State\Return $R_1, \ldots, R_n$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Correctness}
\begin{lemma}
For any path $P$, $prob(P_1, \ldots, P_{n}) \le prob(P_1, \ldots, P_{n-1}), \forall n > 1$.
\end{lemma}
\begin{proof}
By definition, $prob(P_1, \ldots, P_{n}) = prob(P_1, \ldots, P_{i-1}) * T(P_i, P_{i-1}) * E(O_i, P_i)$. Because $T(P_i, P_{i-1})$ and $E(O_i, P_i)$ are probabilities, their product must be on the interval $[0, 1]$. Therefore, $prob(P_1, \ldots, P_{n}) \le prob(P_1, \ldots, P_{i-1})$.
\end{proof}

\begin{theorem}
The max-heap contains the most probable path.
\end{theorem}
\begin{proof}
Suppose, for the sake of contradiction, that there exists a path $T \not\in M$ such that $prob(T) > prob(P), \forall P \in M$. Because $T \not\in M$ and the maximum probability path is chosen each iteration, there must be some time $i < len(T)$ and some path $S \in M$ such that $prob(T_1, \ldots, T_i) \le prob(S)$. If $i = 1$ then $T$ is an initial path. Therefore, $T \in M$. This contradicts the assumption that $T \not\in M$. If $i > 1$ then from Lemma 1.2, $prob(T) \le prob(T_1, \ldots, T_i)$ because $i < len(T)$. Therefore, $prob(T) \le prob(T_1, \ldots, T_i) \le prob(S)$. This contradicts the assumption that $prob(T) > prob(P), \forall P \in M$, because $S \in M$ and $prob(T) \le prob(S)$. Consequently, the max-heap contains the most probable path.
\end{proof}

\begin{theorem}
The algorithm returns the most probable sequence of hidden states.
\end{theorem}
\begin{proof}
Because the max-heap always contains the most probable path (Theorem 1.3) and the maximum probability path is chosen each iteration, the algorithm is guaranteed to return the most probable sequence of hidden states.
\end{proof}

\subsubsection{Performance}
The best case performance is $O(n * \log{|H|})$ in time and $O(n)$ in memory and the worst case performance is equivalent to the Viterbi Algorithm, which is $O(n * |H|^2)$ in time and $O(n * |H|)$ in space, where $n$ is the length of the sequence and $|H|$ is the size of the hidden state space.

\subsection{Generation}
\begin{algorithm}[H]
\caption{Predict the most probable sequence of hidden states $H_1, \ldots, H_n$ that generated a given sequence of observed states $O_1, \ldots, O_n$}
\label{hmm-predict}
\begin{algorithmic}
\Function{Generate}{$t$}
    \State $H = rand(I)$
    \For{$i \in 1, \ldots, t$}
        \State $O_i = rand(E, H)$
        \State $H = rand(T, H)$
    \EndFor
    \State\Return $O_1, \ldots, O_t$
\EndFunction
\end{algorithmic}
\end{algorithm}

\end{document}

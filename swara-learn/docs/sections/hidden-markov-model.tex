\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Hidden Markov Models}

\begin{definition}
A \textbf{hidden markov model} is a supervised learning technique that models a system in which every sequence of observations $O_1, \ldots, O_n$ is generated by a Markov process whose state $H_t$ is hidden from the observer. Hidden markov models are especially useful for sequence prediction (e.g. speech-to-text) and generation (e.g. speech generation). A hidden markov model learns three probability distributions:

\begin{itemize}
    \item \textbf{Initial}: $I = P(H_0)$ \\
    Probability that the initial hidden state is $H_0$.
    \item \textbf{Transition}: $T = P(H_{t+1} | H_{t})$ \\
    Probability of transitioning to hidden state $H_{t+1}$ from hidden state $H_t$.
    \item \textbf{Emission}: $E = P(O_{t} | H_{t})$ \\
    Probability of emitting an observation $O_{t}$ from hidden state $H_{t}$.
\end{itemize}

\end{definition}

\subsection{Prediction}
Given a sequence of observations $O_1, \ldots, O_n$ find the most probable sequence of hidden states $H_1, \ldots, H_n$ that generated them.

\begin{definition}
A \textbf{path} $P$ is a sequence of hidden states $P_1, \ldots, P_n$. For any path $P$, let $len(P) = n$ be the length of the path and $prob(P)$ be the probability of the path such that:
\begin{equation*}
  prob(P)=\begin{cases}
    I(P_1) * E(O_1, P_1), & \text{if $n = 1$}.\\
    prob(P_1, \ldots, P_{n-1}) * T(P_n, P_{n-1}) * E(O_n, P_n), & \text{otherwise}.
  \end{cases}
\end{equation*}
\end{definition}

\subsubsection{Algorithm}
\begin{algorithm}[H]
\caption{Predict the most probable sequence of hidden states $H_1, \ldots, H_n$ that generated a given sequence of observed states $O_1, \ldots, O_n$}
\label{hmm-predict}
\begin{algorithmic}
\Function{Predict}{$O_1, \ldots, O_n$}
    \State $M = $ max-heap of paths keyed by probability
    \State $V = $ set of visited paths
    \ForAll{$H \in I$ s.t. $I(H) > 0$}
        \State $enqueue(H, M)$
    \EndFor
    \While{$R = dequeue(M)$ and $len(R) <= n$}
        \If{$R \not\in V$}
            \ForAll{$H \in T(H, R_{len(R)}) > 0, H \not\in V$}
                \State $enqueue(R \cup H, M)$
                \State $V = V \cup R_{len(R)}$
            \EndFor
        \EndIf
    \EndWhile
    \State\Return $R_1, \ldots, R_n$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Correctness}
\begin{lemma}
\label{diminishing-probability}
For any path $P$ s.t. $len(P) > 1$, $prob(P_1, \ldots, P_{n}) \le prob(P_1, \ldots, P_{n-1})$.
\end{lemma}
\begin{proof}
By definition, $prob(P_1, \ldots, P_{n}) = prob(P_1, \ldots, P_{i-1}) * T(P_i, P_{i-1}) * E(O_i, P_i)$. Because $T(P_i, P_{i-1})$ and $E(O_i, P_i)$ are probabilities, their product must be on the interval $[0, 1]$. Therefore, $prob(P_1, \ldots, P_{n}) \le prob(P_1, \ldots, P_{i-1})$.
\end{proof}

\begin{theorem}
\label{probable-path}
The max-heap contains the most probable path.
\end{theorem}
\begin{proof}
Suppose, for the sake of contradiction, that there exists a path $T \not\in M$ such that $prob(T) > prob(P), \forall P \in M$. Because $T \not\in M$ and the maximum probability path is chosen each iteration, there must be some time $i < len(T)$ and some path $S \in M$ such that $prob(T_1, \ldots, T_i) \le prob(S)$. If $i = 1$ then $T$ is an initial path. Therefore, $T \in M$. This contradicts the assumption that $T \not\in M$. If $i > 1$ then from Lemma \ref{diminishing-probability}, $prob(T) \le prob(T_1, \ldots, T_i)$ because $i < len(T)$. Therefore, $prob(T) \le prob(T_1, \ldots, T_i) \le prob(S)$. This contradicts the assumption that $prob(T) > prob(P), \forall P \in M$, because $S \in M$ and $prob(T) \le prob(S)$. Consequently, the max-heap contains the most probable path.
\end{proof}

\begin{theorem}
The algorithm returns the most probable sequence of hidden states.
\end{theorem}
\begin{proof}
Because the max-heap always contains the most probable path (Theorem \ref{probable-path}) and the maximum probability path is chosen each iteration, the algorithm is guaranteed to return the most probable sequence of hidden states.
\end{proof}

\subsubsection{Performance}
Because removal from a max-heap is $O(\log n)$ and in the best case $n$ removals will be performed to find the optimal path of length $n$, the best case performance is $O(n \log n)$. In the worst case, all possible paths would need to be tried before finding the optimal path of length $n$. Let $|H|$ be the number of hidden states. Because each $H_i$ is visited exactly once in the worst case, the worst case performance is $O(n * |H| * \log |H|)$. This is significantly better than the Viterbi Algorithm which is $O(n * |H|^2)$.

\subsection{Generation}
Let $random(X)$ be a function that returns the value of a weighted random element of $X$ such that each element $(v, w) \in X$ has value $v$ and weight $w$.

\begin{algorithm}[H]
\caption{Generate a sequence of observations $O_1, \ldots, O_t$}
\label{hmm-predict}
\begin{algorithmic}
\Function{Generate}{$t$}
    \State $H_0 = random(\{(h, I(h)): \forall h \in H\})$
    \For{$i \in 1, \ldots, t$}
        \State $O_i = random(\{(o, E(o, H_{i-1})): \forall o \in O\})$
        \State $H_i = random(\{(h, T(h, H_{i-1})): \forall h \in H\})$
    \EndFor
    \State\Return $O_1, \ldots, O_t$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Implementation}
\begin{itemize}
    \item \textbf{Unknown state space}: In many problems, the state space is not known a priori. For example, the state space of all possible chords is infinite because they may contain any number of any note; however, all songs  only ever use a finite number of distinct chords. Traditional dyamic programming algorithms like the Viterbi Algorithm require you to preallocate an $n$ by $|H|$ matrix. This is impossible if the underlying state space $H$ is unknown.
    \item \textbf{Concurrency}: Implementation will be trained on massive datasets, so it would be interesting if it may be used for prediction and generation while it is trained.
\end{itemize}

\end{document}